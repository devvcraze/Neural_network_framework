{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":425599,"sourceType":"datasetVersion","datasetId":190819},{"sourceId":9096281,"sourceType":"datasetVersion","datasetId":5489595},{"sourceId":9152400,"sourceType":"datasetVersion","datasetId":5528730},{"sourceId":1313369,"sourceType":"kernelVersion"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport numpy as np\n\nclass Linear:\n    \n    def __init__(self, in_features, out_features):\n        self.weights = np.random.randn(in_features, out_features) * 0.01\n        self.bias = np.zeros(out_features)\n        self.input = None\n        self.output = None\n\n    def forward(self, x):\n        self.input = x\n        self.output = np.dot(x, self.weights) + self.bias\n        return self.output\n\n    def backward(self, d_output):\n        d_input = np.dot(d_output, self.weights.T)\n        d_weights = np.dot(self.input.T, d_output)\n        d_bias = np.sum(d_output, axis=0)\n        self.weights -= learning_rate * d_weights\n        self.bias -= learning_rate * d_bias\n        return d_input\n\nclass ReLU:\n    def forward(self, x):\n        self.input = x\n        return np.maximum(0, x)\n\n    def backward(self, d_output):\n        return d_output * (self.input > 0)\n\nclass Sigmoid:\n    def forward(self, x):\n        self.input = x\n        self.output = 1 / (1 + np.exp(-x))\n        return self.output\n\n    def backward(self, d_output):\n        return d_output * (self.output * (1 - self.output))\n\nclass Tanh:\n    def forward(self, x):\n        self.input = x\n        self.output = np.tanh(x)\n        return self.output\n\n    def backward(self, d_output):\n        return d_output * (1 - np.power(self.output, 2))\n\nclass Softmax:\n    def forward(self, x):\n        self.input = x\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n        return self.output\n\n    def backward(self, d_output):\n        return d_output\n\nclass CrossEntropyLoss:\n    def forward(self, predictions, targets):\n        self.predictions = predictions\n        self.targets = targets\n        return -np.sum(targets * np.log(predictions + 1e-9)) / predictions.shape[0]\n\n    def backward(self):\n        return (self.predictions - self.targets) / self.targets.shape[0]\n\nclass MSELoss:\n    def forward(self, predictions, targets):\n        self.predictions = predictions\n        self.targets = targets\n        return np.mean((predictions - targets) ** 2)\n\n    def backward(self):\n        return 2 * (self.predictions - self.targets) / self.targets.size\n\nclass SGD:\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def step(self, layer):\n        layer.weights -= self.learning_rate * layer.d_weights\n        layer.bias -= self.learning_rate * layer.d_bias\n\nclass Model:\n    def __init__(self):\n        self.layers = []\n        self.loss_function = None\n        self.optimizer = None\n\n    def add_layer(self, layer):\n        self.layers.append(layer)\n\n    def compile(self, loss_function, optimizer):\n        self.loss_function = loss_function\n        self.optimizer = optimizer\n\n    def forward(self, x):\n        self.activations = []\n        self.inputs = [x]\n        for layer in self.layers:\n            x = layer.forward(x)\n            self.inputs.append(x)\n            self.activations.append(layer)\n        return x\n\n    def backward(self, d_output):\n        for layer in reversed(self.activations):\n            d_output = layer.backward(d_output)\n            if isinstance(layer, Linear):\n                self.optimizer.step(layer)\n\n    def train(self, x_train, y_train, epochs, batch_size):\n        for epoch in range(epochs):\n            for start in range(0, x_train.shape[0], batch_size):\n                end = start + batch_size\n                x_batch, y_batch = x_train[start:end], y_train[start:end]\n                predictions = self.forward(x_batch)\n                loss = self.loss_function.forward(predictions, y_batch)\n                d_loss = self.loss_function.backward()\n                self.backward(d_loss)\n            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n\n    def evaluate(self, x_test, y_test):\n        predictions = self.forward(x_test)\n        loss = self.loss_function.forward(predictions, y_test)\n        accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n        return loss, accuracy\n\n    def save(self, path):\n        np.savez(path, weights=[layer.weights for layer in self.layers if isinstance(layer, Linear)],\n                 bias=[layer.bias for layer in self.layers if isinstance(layer, Linear)])\n\n    def load(self, path):\n        data = np.load(path)\n        weights = data['weights']\n        bias = data['bias']\n        idx = 0\n        for layer in self.layers:\n            if isinstance(layer, Linear):\n                layer.weights = weights[idx]\n                layer.bias = bias[idx]\n                idx += 1\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:09:32.380666Z","iopub.execute_input":"2024-08-12T17:09:32.381175Z","iopub.status.idle":"2024-08-12T17:09:32.415522Z","shell.execute_reply.started":"2024-08-12T17:09:32.381138Z","shell.execute_reply":"2024-08-12T17:09:32.414335Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = Model()\nmodel.add_layer(Linear(784, 128))\nmodel.add_layer(ReLU())\nmodel.add_layer(Linear(128, 10))\nmodel.add_layer(Softmax())\n\n# Compile the model with loss and optimizer\nloss = CrossEntropyLoss()\noptimizer = SGD(learning_rate=0.01)\nmodel.compile(loss, optimizer)\n\n# Assume x_train, y_train, x_test, y_test are preprocessed and available\n# Train the model\nmodel.train(x_train, y_train, epochs=20, batch_size=64)\n\n# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(x_test, y_test)\nprint(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:20:57.543252Z","iopub.execute_input":"2024-08-12T17:20:57.543716Z","iopub.status.idle":"2024-08-12T17:20:57.817545Z","shell.execute_reply.started":"2024-08-12T17:20:57.543686Z","shell.execute_reply":"2024-08-12T17:20:57.816057Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss, optimizer)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Assume x_train, y_train, x_test, y_test are preprocessed and available\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(x_test, y_test)\n","Cell \u001b[0;32mIn[10], line 130\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, x_train, y_train, epochs, batch_size)\u001b[0m\n\u001b[1;32m    128\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function\u001b[38;5;241m.\u001b[39mforward(predictions, y_batch)\n\u001b[1;32m    129\u001b[0m     d_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_function\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","Cell \u001b[0;32mIn[10], line 118\u001b[0m, in \u001b[0;36mModel.backward\u001b[0;34m(self, d_output)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_output):\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations):\n\u001b[0;32m--> 118\u001b[0m         d_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, Linear):\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep(layer)\n","Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mLinear.backward\u001b[0;34m(self, d_output)\u001b[0m\n\u001b[1;32m     18\u001b[0m d_weights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput\u001b[38;5;241m.\u001b[39mT, d_output)\n\u001b[1;32m     19\u001b[0m d_bias \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(d_output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mlearning_rate\u001b[49m \u001b[38;5;241m*\u001b[39m d_weights\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m d_bias\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d_input\n","\u001b[0;31mNameError\u001b[0m: name 'learning_rate' is not defined"],"ename":"NameError","evalue":"name 'learning_rate' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}