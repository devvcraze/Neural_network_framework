{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":425599,"sourceType":"datasetVersion","datasetId":190819},{"sourceId":9096281,"sourceType":"datasetVersion","datasetId":5489595},{"sourceId":9152400,"sourceType":"datasetVersion","datasetId":5528730},{"sourceId":1313369,"sourceType":"kernelVersion"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport numpy as np\n\nclass Linear:\n    \n    def __init__(self, in_features, out_features):\n        self.weights = np.random.randn(in_features, out_features) * 0.01\n        self.bias = np.zeros(out_features)\n        self.input = None\n        self.output = None\n\n    def forward(self, x):\n        self.input = x\n        self.output = np.dot(x, self.weights) + self.bias\n        return self.output\n\n    def backward(self, d_output):\n        d_input = np.dot(d_output, self.weights.T)\n        d_weights = np.dot(self.input.T, d_output)\n        d_bias = np.sum(d_output, axis=0)\n        self.weights -= learning_rate * d_weights\n        self.bias -= learning_rate * d_bias\n        return d_input\n\nclass ReLU:\n    def forward(self, x):\n        self.input = x\n        return np.maximum(0, x)\n\n    def backward(self, d_output):\n        return d_output * (self.input > 0)\n\nclass Sigmoid:\n    def forward(self, x):\n        self.input = x\n        self.output = 1 / (1 + np.exp(-x))\n        return self.output\n\n    def backward(self, d_output):\n        return d_output * (self.output * (1 - self.output))\n\nclass Tanh:\n    def forward(self, x):\n        self.input = x\n        self.output = np.tanh(x)\n        return self.output\n\n    def backward(self, d_output):\n        return d_output * (1 - np.power(self.output, 2))\n\nclass Softmax:\n    def forward(self, x):\n        self.input = x\n        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n        self.output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n        return self.output\n\n    def backward(self, d_output):\n        return d_output\n\nclass CrossEntropyLoss:\n    def forward(self, predictions, targets):\n        self.predictions = predictions\n        self.targets = targets\n        return -np.sum(targets * np.log(predictions + 1e-9)) / predictions.shape[0]\n\n    def backward(self):\n        return (self.predictions - self.targets) / self.targets.shape[0]\n\nclass MSELoss:\n    def forward(self, predictions, targets):\n        self.predictions = predictions\n        self.targets = targets\n        return np.mean((predictions - targets) ** 2)\n\n    def backward(self):\n        return 2 * (self.predictions - self.targets) / self.targets.size\n\nclass SGD:\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n\n    def step(self, layer):\n        layer.weights -= self.learning_rate * layer.d_weights\n        layer.bias -= self.learning_rate * layer.d_bias\n\nclass Model:\n    def __init__(self):\n        self.layers = []\n        self.loss_function = None\n        self.optimizer = None\n\n    def add_layer(self, layer):\n        self.layers.append(layer)\n\n    def compile(self, loss_function, optimizer):\n        self.loss_function = loss_function\n        self.optimizer = optimizer\n\n    def forward(self, x):\n        self.activations = []\n        self.inputs = [x]\n        for layer in self.layers:\n            x = layer.forward(x)\n            self.inputs.append(x)\n            self.activations.append(layer)\n        return x\n\n    def backward(self, d_output):\n        for layer in reversed(self.activations):\n            d_output = layer.backward(d_output)\n            if isinstance(layer, Linear):\n                self.optimizer.step(layer)\n\n    def train(self, x_train, y_train, epochs, batch_size):\n        for epoch in range(epochs):\n            for start in range(0, x_train.shape[0], batch_size):\n                end = start + batch_size\n                x_batch, y_batch = x_train[start:end], y_train[start:end]\n                predictions = self.forward(x_batch)\n                loss = self.loss_function.forward(predictions, y_batch)\n                d_loss = self.loss_function.backward()\n                self.backward(d_loss)\n            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n\n    def evaluate(self, x_test, y_test):\n        predictions = self.forward(x_test)\n        loss = self.loss_function.forward(predictions, y_test)\n        accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n        return loss, accuracy\n\n    def save(self, path):\n        np.savez(path, weights=[layer.weights for layer in self.layers if isinstance(layer, Linear)],\n                 bias=[layer.bias for layer in self.layers if isinstance(layer, Linear)])\n\n    def load(self, path):\n        data = np.load(path)\n        weights = data['weights']\n        bias = data['bias']\n        idx = 0\n        for layer in self.layers:\n            if isinstance(layer, Linear):\n                layer.weights = weights[idx]\n                layer.bias = bias[idx]\n                idx += 1\n","metadata":{"execution":{"iopub.status.busy":"2024-08-12T17:09:32.380666Z","iopub.execute_input":"2024-08-12T17:09:32.381175Z","iopub.status.idle":"2024-08-12T17:09:32.415522Z","shell.execute_reply.started":"2024-08-12T17:09:32.381138Z","shell.execute_reply":"2024-08-12T17:09:32.414335Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}